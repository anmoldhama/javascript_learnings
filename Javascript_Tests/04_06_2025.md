✅ Section 1: Node.js Architecture & Event Loop Deep Dive (10 Questions)
Q) What are the differences between microtasks and macrotasks in Node.js?

✅ Q2) How does Node.js handle I/O under the hood, and what is epoll?
Answer:

Node.js uses libuv, a C library, to handle asynchronous I/O operations.

For Linux systems, libuv uses epoll, a kernel-level system call that efficiently waits for multiple file descriptors to become "ready" for I/O.

This enables non-blocking I/O, allowing Node.js to use a single-threaded event loop efficiently.

🔍 epoll waits for multiple events like "file is ready to read", "socket is ready to write", etc., and notifies Node.js so it can process them.

✅ Q3) How does Node.js avoid blocking while handling file I/O or DNS lookups?
Answer:

Node.js uses libuv's thread pool to offload blocking operations like:

File system access (e.g., fs.readFile)

DNS lookups (e.g., dns.lookup)

These run in the libuv thread pool (4 threads by default), not on the main event loop.

Once done, a callback is queued into the event loop to be executed asynchronously.

✅ This prevents the main thread from blocking and keeps Node.js responsive.

✅ Q4) What is the relationship between the Node.js event loop and the libuv thread pool?
Answer:

Event Loop runs on the main single thread in Node.js.

Thread Pool (part of libuv) is used for running blocking tasks in background threads.

File system I/O

DNS resolution

Compression (zlib), crypto, etc.

📌 The thread pool handles these tasks asynchronously and pushes the results back into the event loop when ready.

✅ Q5) What is starvation in the event loop, and how do you avoid it?
Answer:

Starvation occurs when long-running or too many microtasks prevent macrotasks or I/O callbacks from executing.

Example:

js
Copy
Edit
function floodMicrotasks() {
  process.nextTick(floodMicrotasks); // blocks event loop
}
floodMicrotasks();
This blocks timers, I/O, and everything else!

✅ How to avoid it:

Avoid unbounded recursive process.nextTick() or Promise.resolve().then() calls.

Break large tasks into smaller chunks using setImmediate() or setTimeout(fn, 0).

✅ Q6) What are the phases of the Node.js event loop and what executes in each?
Answer:

The Node.js event loop has 6 main phases:

Timers

Executes callbacks scheduled by setTimeout() and setInterval().

Pending Callbacks

Executes I/O callbacks that were deferred to the next loop.

Idle, Prepare

Internal use only.

Poll

Waits for new I/O events, executes I/O callbacks (e.g., fs.readFile), and handles incoming data.

Check

Executes setImmediate() callbacks.

Close Callbacks

Executes close events like socket.on('close', ...).

🔁 Between each phase, Node.js processes microtasks (Promise.then, process.nextTick).

✅ Q7) How does Node.js handle backpressure in streams?
Answer:

Backpressure happens when the writable side of a stream cannot consume data as fast as the readable side produces it.

Node.js handles this by:

Returning false from writable.write() to signal "stop writing".

The readable stream pauses automatically.

It resumes when the 'drain' event is emitted.

js
Copy
Edit
const readable = fs.createReadStream('bigfile');
const writable = fs.createWriteStream('output');

readable.pipe(writable);
This automatically handles backpressure via .pipe() under the hood.

✅ Q8) Explain how process.nextTick() differs from setImmediate() and setTimeout().
Function	When it runs	Priority
process.nextTick()	Before any I/O and after current stack	Highest
setImmediate()	After I/O phase	Lower
setTimeout()	After timer expires (≥ specified ms delay)	Lowest

🔍 Use case:

Use process.nextTick() to defer execution until after current stack, but before I/O.

Use setImmediate() to defer until after current I/O events.

✅ Q9) How does Node.js prioritize tasks from various queues (Timers, I/O callbacks, etc.)?
Answer:

Order of execution in Node.js is:

markdown
Copy
Edit
1. current JS call stack
2. process.nextTick queue (microtasks)
3. Promises queue (microtasks)
4. Event loop phases:
   - timers → pending callbacks → poll → check → close callbacks
5. Between phases: process microtasks again
💡 Within each phase, tasks are FIFO, but microtasks always run between phases and have higher priority.

✅ Q10) What is the purpose of the uv_run() loop in libuv, and how does it affect Node.js behavior?
Answer:

uv_run() is the core loop in libuv that powers Node.js’s event loop.

It:

Polls for I/O readiness (epoll, kqueue, etc.)

Executes ready callbacks (timers, I/O, etc.)

Invokes microtask queues when needed

Think of it like:

c
Copy
Edit
while (there are tasks) {
  run_timers();
  run_pending_callbacks();
  poll_for_events();
  run_idle();
  run_check();
  close_callbacks();                               
}
📌 uv_run() drives the entire lifecycle of asynchronous execution in Node.js.



✅ Section 2: Worker Threads & Clustering (7 Questions)
Q) How do worker threads improve Node.js performance for CPU-intensive tasks?
ans : worker threads executes the cpu-intensive task seperated to the main thread 
      but shared the same state of resources.
      It will help to avoid the blocking of the main thread.

Q) What is the difference between child processes and worker threads in Node.js?
ans : child processes is used to execute a shell script or command in the isolation to the main thread
      and it will not share the same state and resources.

Q) When should you use the cluster module vs worker threads in a Node.js app?
ans : clustering helps to create multiple instances of the same application running on the same port
      worker threads are used to execute the cpu intensive tasks.

Q) How do you share data between worker threads in Node.js?
ans : worker threads using IPC to share data between

Q) How do message channels or SharedArrayBuffer work in the context of worker threads?
ans :

MessageChannel:

Provides two-way communication between the main thread and worker threads.

You create a channel with new MessageChannel(), which gives two ports: port1 and port2.

You can pass one of the ports to the worker using worker.postMessage() with transferList, enabling bidirectional messaging.

const { Worker, MessageChannel } = require('worker_threads');
const worker = new Worker('./worker.js');
const { port1, port2 } = new MessageChannel();

worker.postMessage({ port: port1 }, [port1]);

port2.on('message', (msg) => {
  console.log('From worker:', msg);
});
SharedArrayBuffer:

Enables zero-copy memory sharing between threads.

Unlike postMessage, which serializes data, SharedArrayBuffer allows threads to access the same memory location.

Typically used with Atomics to prevent race conditions.

const shared = new SharedArrayBuffer(4); // 4 bytes = Int32Array of 1 element
const sharedArray = new Int32Array(shared);
📌 Use SharedArrayBuffer for performance-critical applications (e.g. large shared datasets, audio processing, simulations).

Q) What challenges arise when clustering a stateful Node.js application?  implement  
ans : done
Clustering introduces multi-process architecture, which creates state-sharing challenges:

Session/state consistency:

Each cluster has its own memory. If a user session is stored in memory (e.g. in-memory cache), it won’t be available to other clusters.

✅ Use Redis or DB-backed session stores for shared state.

Sticky sessions:

Load balancers may route requests randomly. This causes inconsistency in session handling.

✅ Use sticky sessions or session replication.

Race conditions:

Concurrent updates to shared resources can conflict.

✅ Use locking mechanisms or event queues.

Increased complexity:

Debugging, logging, and error handling are harder in multi-process environments.

Q) How would you gracefully shut down a worker thread during a deployment or crash?
ans :
To gracefully shut down a worker thread:

Listen for shutdown signals:

e.g. SIGINT, SIGTERM

Send shutdown message to worker:

Use worker.postMessage({ type: 'shutdown' })

Inside the worker:

Clean up resources and call process.exit() or allow the event loop to empty.

Main thread waits for exit event:

Ensures the worker has exited before proceeding.

// Main thread
worker.postMessage({ type: 'shutdown' });
worker.once('exit', () => {
  console.log('Worker has exited gracefully');
});

// Worker thread
parentPort.on('message', (msg) => {
  if (msg.type === 'shutdown') {
    // Clean up resources
    process.exit(0);
  }
});
📌 Always avoid worker.terminate() unless in an emergency — it force-kills the thread without cleanup.

✅ Section 3: Streams & Buffers (7 Questions)
Q) What are the different types of streams in Node.js?
ans : done

Q) How do you pipe streams together and handle errors across multiple stream layers?  implement
ans : 
const fs = require('fs');
const zlib = require('zlib');
const { pipeline } = require('stream');
const util = require('util');

const pipe = util.promisify(pipeline); // optional: to use with async/await

async function compressFile() {
  try {
    await pipe(
      fs.createReadStream('input.txt'),
      zlib.createGzip(),
      fs.createWriteStream('input.txt.gz')
    );
    console.log('Compression successful');
  } catch (err) {
    console.error('Pipeline failed:', err);
  }
}

compressFile();

pipeline is better than .pipe() because it gives the error handling easy

Q) What is the significance of highWaterMark in streams?
ans : 
highWaterMark is a buffer threshold that determines how much data can be stored in memory before a stream pauses reading or writing.
It helps control backpressure in streams to avoid overwhelming the system with too much data at once.

const fs = require('fs');

const readStream = fs.createReadStream('large.txt', {
  highWaterMark: 1024 * 32 // 32 KB buffer instead of default 64 KB
});


Q) How does Node.js handle binary data with Buffers, and what encoding issues can occur?
ans : 
A Buffer is a built-in object in Node.js used to handle raw binary data directly in memory, especially useful for:

File system I/O

TCP sockets

Streams

Binary protocols

Buffers are fixed-size chunks of memory — allocated outside the V8 heap.

issues :
1. Partial multi-byte characters
2. Wrong encoding interpretation
3. Truncation / overflow

Q) How can you compress a large file using streams and zlib?
ans : 
Efficiently compress a large file without loading it entirely into memory, using Node.js streams and the zlib module.
 1. Use zlib.createGzip() for compression
 2. Use fs.createReadStream() and fs.createWriteStream()


const fs = require('fs');
const zlib = require('zlib');
const { pipeline } = require('stream');
const util = require('util');

const pipe = util.promisify(pipeline);

async function compressFile(inputPath, outputPath) {
  try {
    await pipe(
      fs.createReadStream(inputPath),
      zlib.createGzip(), // transform stream for compression
      fs.createWriteStream(outputPath)
    );
    console.log(`✅ Compression successful: ${outputPath}`);
  } catch (err) {
    console.error('❌ Compression failed:', err);
  }
}

// Example usage:
compressFile('large-file.txt', 'large-file.txt.gz');


Q) How does Readable.from() work and what use-cases does it solve?
ans :
Readable.from() is a static method in Node.js (v10+) that creates a Readable stream from any iterable or async iterable source.

It's a quick and clean way to turn data structures like arrays, generators, or async APIs into streamable data.

Q) How do you transform stream data in chunks while keeping memory usage low?

✅ Section 4: Advanced Error Handling (6 Questions)
Q) How do you handle uncaught exceptions and unhandled promise rejections in production?

Q) What’s the difference between operational errors and programmer errors in Node.js?
ans : operational error are coming by the crashes or something stop working
     programmer error are coming by the mistake of programmer like the syntex error.

Q) How would you implement a global error handler in an Express application? 
ans : using 4 parameter middleware
// errorHandler.js
function globalErrorHandler(err, req, res, next) {
  console.error('Error:', err.stack || err.message);

  const statusCode = err.statusCode || 500;

  res.status(statusCode).json({
    success: false,
    message: err.message || 'Internal Server Error',
    stack: process.env.NODE_ENV === 'production' ? undefined : err.stack,
  });
}

module.exports = globalErrorHandler;

and call it in main file as app.use();


Q) Why is it bad to throw inside an async function and how to avoid it?
ans : 

Q) How can domains or async_hooks be used for advanced error tracing?

Q) What happens if you forget to return a promise inside a middleware?

✅ Section 5: Real-World Best Practices (8 Questions)
Q) How do you detect and prevent slow HTTP attacks (e.g., Slowloris) in Node.js?

Q) What are the best practices for validating user input in a Node.js API?

Q) Why is using bcrypt in synchronous mode dangerous in Node.js apps?

Q) How do you structure a large Node.js monorepo using tools like Lerna or Turborepo?

Q) What logging strategy would you use in a distributed Node.js system?

Q) Why should you avoid using require dynamically inside functions?

Q) What are the best practices for implementing graceful shutdown in Node.js?

Q) How do you decide between using a message queue vs REST API call for inter-service communication?

✅ Section 6: Testing & Code Quality (6 Questions)
Q) How do you test a function that includes timeouts or intervals?

Q) What is mutation testing and how can it improve test coverage in Node.js?

Q) How do you mock dependencies like file system or database in unit tests?

Q) How do you achieve test parallelism in Node.js while avoiding shared-state issues?

Q) What is the difference between stubbing, mocking, and spying in a Node.js test?

Q) How do you implement test hooks and lifecycle methods using Mocha or Jest?

✅ Section 7: Observability & Production Debugging (6 Questions)
Q) How do you implement distributed tracing in a Node.js microservices setup?

Q) How would you handle a memory leak in a production Node.js app?

Q) What metrics are essential for monitoring a Node.js service?

Q) How do you use clinic.js to diagnose performance issues?

Q) What tools help analyze flame graphs or CPU profiles in Node.js?

Q) How do you use async_hooks for context propagation in observability setups?

